---
title: DirectAccess の容量計画
description: このトピックでは、Windows Server 2016 での DirectAccess の容量計画を支援するのに Windows Server 2012 の DirectAccess サーバーのパフォーマンスに関するレポートを表示使用できます。
manager: brianlic
ms.custom: na
ms.prod: windows-server-threshold
ms.reviewer: na
ms.suite: na
ms.technology: networking-da
ms.tgt_pltfrm: na
ms.topic: article
ms.assetid: 456e5971-3aa7-4a24-bc5d-0c21fec7687e
ms.author: pashort
author: shortpatti
ms.openlocfilehash: d6f0f4a089dd8e99bb9f9815f0900a3c53c9d1ba
ms.sourcegitcommit: afb0602767de64a76aaf9ce6a60d2f0e78efb78b
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/20/2019
ms.locfileid: "67281984"
---
# <a name="directaccess-capacity-planning"></a>DirectAccess の容量計画

>適用先:Windows Server 2016 の Windows Server (半期チャネル)

このドキュメントは、Windows Server 2012 の DirectAccess サーバーのパフォーマンスに関するレポートです。 テストはハイエンドのコンピューター ハードウェアとローエンドのコンピューター ハードウェアのスループット容量を特定するために行われました。 ハイエンドとローエンドの CPU のパフォーマンスは、ネットワーク トラフィックのスループットと使用するクライアントの種類に依存していました。 DirectAccess の一般的な展開 (およびこれらのテストのベース) は、1/3 (30%) が IPHTTPS クライアント、2/3 (70%) が Teredo クライアントで構成されています。 Teredo クライアントが IPHTTPS クライアントよりも多いのは、Windows Server 2012 ではすべての CPU コアの使用が許可される Receive Side Scaling (RSS) が使用されるためです。 これらのテストでは、RSS が有効になるため、ハイパー スレッディングは無効になっています。 また、Windows Server 2012 の TCP/IP は UDP トラフィックをサポートするため、Teredo クライアントでは CPU 間で負荷を分散できます。  
  
データは、ローエンド (4 コア、4 ギガ) サーバーと、ハイエンド (8 コア、8 ギガ) サーバーでより一般的であると期待されるハードウェアの両方から収集されました。  750 台のクライアント (Teredo 562 台、iphttps 188 台) を持つローエンド ハードウェアで新しい Windows 8 タスク マネージャーのスクリーン ショットを次に示します ~ 77 メガビット/秒を実行します。これは、スマート カードの資格情報を提示しないユーザーをシミュレートします。  
  
これらのテスト結果は、Windows 8 では Teredo が IPHTTPS よりもパフォーマンスが高いことを示しています。ただし、Windows 7 と比べると、Teredo と IPHTTPS のいずれも帯域幅を使用効率が向上したことを示しています。  
  
![テスト結果](../../media/DirectAccess-Capacity-Planning/DACapacityPlanning1.gif)  
  
## <a name="high-end-hardware-test-environment"></a>ハイエンド ハードウェアのテスト環境  
次のグラフは、ハイエンド ハードウェアのパフォーマンス テスト環境の結果を示しています。 すべてのテスト結果と分析は、このドキュメントで詳しく説明します。  
  
||||  
|-|-|-|  
|構成 - ハードウェア|ローエンド ハードウェア (4 GB RAM、4 コア)|ハイエンド ハードウェア (8 GB、8 コア)|  
|ダブル トンネル<br /><br />-PKI<br /><br />(Dns64/nat64 を含む)|750 の同時接続、50% CPU、50% メモリ、Corpnet NIC スループット 75 Mbps。 拡大のターゲットは 1,000 ユーザー @ 50% CPU です。|1500 の同時接続、50% CPU、50 % メモリ、Corpnet NIC スループット 150 Mbps。|  
## <a name="test-environment"></a>テスト環境

**Perf ブランチ トポロジ**  
  
![テスト環境](../../media/DirectAccess-Capacity-Planning/DACapacityPlanning2.gif)  
  
パフォーマンス テスト環境は、5 台のコンピューターのベンチです。 ローエンドのテストでは、1 台の 4 コア 4 ギガの DirectAccess サーバーが使用され、ハイエンド ハードウェアのテストでは、1 台の 8 コア、16 ギガの DirectAccess サーバーが使用されました。 ローエンドとハイエンドのテスト環境で、1 台のバックエンド サーバー (送信側) と、2 台のクライアント コンピューター (受信側) が使用されました。  受信側は 2 台のクライアント コンピューターに分割されます。 それ以外は、受信側は CPU にバインドされ、クライアント数と帯域幅を制限します。 受信側では、シミュレーターによって多数のクライアントをシミュレートします (HTTPS または Teredo クライアントはシミュレートされます)。 IPsec、DOSp はいずれも構成されています。 RSS は、DirectAccess サーバー上で有効になっています。 RSS のキューのサイズは 8 に設定されています。  RSS を構成しない場合、1 つのプロセッサの使用率が高いままになる一方で、他のコアの使用率は低くなります。 また、注目すべき点は、この DirectAccess サーバーがハイパー スレッディングを無効にした、4 コアのコンピューターであるということです。  ハイパー スレッディングを無効にするのは、RSS が物理コアについてのみ動作し、ハイパー スレッディングを使用すると結果が歪曲されるためです (つまり、すべてのコアの負荷が均一にならないことを意味します)。  
  
## <a name="testing-results-for-low-end-hardware"></a>ローエンド ハードウェアのテスト結果:

テストは 1,000 台のクライアントと 750 台のクライアントの両方について実施されました。  どちらの場合もトラフィックの分割は、Teredo が 70% で、IPHTTPS が 30% です。  すべてのテストは、クライアントごとに 2 つの IPsec トンネルを使用する、Nat64 上の TCP トラフィックについて行われました。  すべてのテストで、メモリの使用量は少なく、CPU 使用率は許容範囲でした。  
  
**個々 のテスト結果:**  
  
以降のセクションでは、個々のテストについて説明します。 各セクション タイトルは各テストの重要な要素を示しており、その後に結果の概要説明と、詳細な結果データのグラフを示します。  
  
**ローエンド パフォーマンス:750 台のクライアント、70/30 の分割、84.17 メガ ビット/秒のスループット:**  
  
次の 3 つのテストは、ローエンド ハードウェアを表します。  以下のテスト実行では、750 台のクライアントを 84.17 Mbps のスループットで使用し、トラフィックは 562 台の Teredo および 188 台の IPHTTPS に分割されます。 Teredo MTU は 1472 に設定され、Teredo 分路は有効になっています。 CPU 使用率は 3 つのテストの平均で 46.42% であり、平均メモリ使用率 (4 GB の使用可能なメモリの合計に対するコミット バイトのパーセンテージで表される) は 25.95% です。  
  
||||||||  
|-|-|-|-|-|-|-|  
|**Scenario**|**CPUAvg (カウンター)**|**M ビット/秒 (Corp 側)**|**M ビット/秒 (インターネット側)**|**アクティブ QMSA**|**アクティブ MMSA**|**メモリ使用率 (4 ギガのシステム)**|  
|**ローエンド HW。562 台の Teredo クライアント。188 台の IPHTTPS クライアント。**|47.7472542|84.3|119.13|1502.05|1502.1|26.27%|  
|**ローエンド HW。562 台の Teredo クライアント。188 台の IPHTTPS クライアント。**|46.3889778|84.146|118.73|1501.25|1501.2|25.90%|  
|**ローエンド HW。562 台の Teredo クライアント。188 台の IPHTTPS クライアント。**|45.113082|84.0494|118.43|1546.14|1546.1|25.68%|  
  
**1,000 台のクライアント、70/30 の分割、78 メガビット/秒のスループット:**  
  
次の 3 つのテストは、ローエンド ハードウェアでのパフォーマンスを表します。 以下のテスト実行では、1,000 台のクライアントを～78.64 Mbps の平均スループットで使用し、トラフィックは 700 台の Teredo および 300 台の IPHTTPS に分割されます。  Teredo MTU は 1472 に設定され、Teredo 分路は有効になっています。  CPU 使用率は平均で～ 50.7% であり、平均メモリ使用率 (4 GB の使用可能なメモリの合計に対するコミット バイトのパーセンテージで表される) は～ 28.7% です。  
  
||||||||  
|-|-|-|-|-|-|-|  
|**Scenario**|**CPUAvg (カウンター)**|**M ビット/秒 (Corp 側)**|**M ビット/秒 (インターネット側)**|**アクティブ QMSA**|**アクティブ MMSA**|**メモリ使用率 (4 ギガのシステム)**|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|51.28406247|78.6432|113.19|2002.42|1502.1|25.59%|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|51.06993128|78.6402|113.22|2001.4|1501.2|30.87%|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|49.75235617|78.6387|113.2|2002.6|1546.1|30.66%|  
  
**1,000 台のクライアント、70/30 の分割、109 メガ ビット/秒のスループット:**  
  
以下の 3 つのテスト実行では、1,000 台のクライアントを～109.2 Mbps の平均スループットで使用し、トラフィックは 700 台の Teredo および 300 台の IPHTTPS に分割されます。 Teredo MTU は 1472 に設定され、Teredo 分路は有効になっています。 CPU 使用率は 3 つのテストの平均で～ 59.06% であり、平均メモリ使用率 (4 GB の使用可能なメモリの合計に対するコミット バイトのパーセンテージで表される) は～ 27.34% です。  
  
||||||||  
|-|-|-|-|-|-|-|  
|**Scenario**|**CPUAvg (カウンター)**|**M ビット/秒 (Corp 側)**|**M ビット/秒 (インターネット側)**|**アクティブ QMSA**|**アクティブ MMSA**|**メモリ使用率 (4 ギガのシステム)**|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|59.81640675|108.305|153.14|2001.64|2001.6|24.38%|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|59.46473798|110.969|157.53|2005.22|2005.2|28.72%|  
|**ローエンド HW。700 台の Teredo クライアント。300 台の IPHTTPS クライアント。**|57.89089768|108.305|153.14|1999.53|2018.3|24.38%|  
  
## <a name="testing-results-for-high-end-hardware"></a>ハイエンド ハードウェアのテスト結果:  
テストは 1,500 台のクライアントについて実施されました。 トラフィックの分割は、Teredo が 70% で、IPHTTPS が 30% です。 すべてのテストは、クライアントごとに 2 つの IPsec トンネルを使用する、Nat64 上の TCP トラフィックについて行われました。 すべてのテストで、メモリの使用量は少なく、CPU 使用率は許容範囲でした。  
  
**個々 のテスト結果:**  
  
以降のセクションでは、個々のテストについて説明します。 各セクション タイトルは各テストの重要な要素を示しており、その後に結果の概要説明と、詳細な結果データのグラフを示します。  
  
**1,500 台のクライアント、70/30 の分割、153.2 メガビット/秒のスループット**  
  
次の 5 つのテストは、ハイエンド ハードウェアを表します。 以下のテスト実行では、1,500 台のクライアントを～ 153.2 Mbps の平均スループットで使用し、トラフィックは 1,050 台の Teredo および 450 台の IPHTTPS に分割されます。 CPU 使用率は 5 つのテストの平均で 50.68% であり、平均メモリ使用率 (8 GB の使用可能なメモリの合計に対するコミット バイトのパーセンテージで表される) は 22.25% です。  
  
||||||||  
|-|-|-|-|-|-|-|  
|**Scenario**|**CPUAvg (カウンター)**|**M ビット/秒 (Corp 側)**|**M ビット/秒 (インターネット側)**|**アクティブ QMSA**|**アクティブ MMSA**|**メモリ使用率 (4 ギガのシステム)**|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|51.712437|157.029|216.29|3000.31|3046|21.58%|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|48.86020205|151.012|206.53|3002.86|3045.3|21.15%|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|52.23979519|155.511|213.45|3001.15|3002.9|22.90%|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|51.26269767|155.09|212.92|3000.74|3002.4|22.91%|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|50.15751307|154.772|211.92|3000.9|3002.1|22.93%|  
|**ハイエンド HW。1,050 台の Teredo クライアント。450 台の IPHTTPS クライアント。**|49.83665607|145.994|201.92|3000.51|3006|22.03%|  
  
![ハイエンド ハードウェアのテスト結果](../../media/DirectAccess-Capacity-Planning/DACapacityPlanning3.gif)  
  


